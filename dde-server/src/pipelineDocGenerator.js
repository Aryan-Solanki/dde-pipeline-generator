/**
 * Pipeline Documentation Generator
 * Generates comprehensive README documentation for individual pipelines
 */

/**
 * Generate a README.md for a specific pipeline
 * @param {Object} dag - The DAG specification
 * @param {Object} metadata - Additional metadata about the pipeline
 * @returns {string} Markdown documentation
 */
export function generatePipelineReadme(dag, metadata = {}) {
  const {
    dag_id,
    description,
    schedule_interval,
    start_date,
    tags = [],
    tasks = [],
    default_args = {}
  } = dag;

  const readme = `# ${dag_id}

> ${description || 'Airflow DAG Pipeline'}

## Overview

**DAG ID**: \`${dag_id}\`  
**Schedule**: \`${schedule_interval || '@daily'}\`  
**Start Date**: \`${start_date || 'Current date'}\`  
**Tags**: ${tags.length > 0 ? tags.map(t => `\`${t}\``).join(', ') : 'None'}

${description ? `\n### Description\n\n${description}\n` : ''}

## Pipeline Structure

This pipeline consists of **${tasks.length} task${tasks.length !== 1 ? 's' : ''}**:

${tasks.map((task, index) => `
### ${index + 1}. ${task.task_id}

**Operator**: \`${task.operator}\`  
${task.description ? `**Description**: ${task.description}\n` : ''}
${generateTaskDetails(task)}
`).join('\n')}

## Task Dependencies

\`\`\`
${generateDependencyGraph(tasks, dag.dependencies)}
\`\`\`

## Configuration

### Default Arguments

\`\`\`python
default_args = {
${Object.entries(default_args).map(([key, value]) => `    '${key}': ${JSON.stringify(value)},`).join('\n')}
}
\`\`\`

### Environment Variables

The following environment variables should be configured in Airflow:

${extractEnvironmentVariables(tasks).map(env => `- \`${env.name}\`: ${env.description}`).join('\n') || '- No environment variables required'}

### Connections

The following Airflow connections are required:

${extractConnections(tasks).map(conn => `- **\`${conn.id}\`**: ${conn.type} connection${conn.description ? ` - ${conn.description}` : ''}`).join('\n') || '- No connections required'}

## Installation

### 1. Prerequisites

- Apache Airflow 2.0 or higher
- Python 3.8 or higher
- Required packages (see requirements.txt)

### 2. Deploy to Airflow

\`\`\`bash
# Copy DAG file to Airflow DAGs folder
cp ${dag_id}.py $AIRFLOW_HOME/dags/

# Verify DAG is recognized
airflow dags list | grep ${dag_id}

# Test DAG parsing
airflow dags list-import-errors
\`\`\`

### 3. Configure Connections

\`\`\`bash
${extractConnections(tasks).map(conn => 
`# Add ${conn.type} connection
airflow connections add '${conn.id}' \\
    --conn-type '${conn.type}' \\
    --conn-host 'your-host' \\
    --conn-login 'your-username' \\
    --conn-password 'your-password'`
).join('\n\n') || '# No connections to configure'}
\`\`\`

### 4. Test the Pipeline

\`\`\`bash
# Test a single task
airflow tasks test ${dag_id} ${tasks[0]?.task_id || 'first_task'} 2026-02-06

# Trigger the DAG
airflow dags trigger ${dag_id}

# Monitor execution
airflow dags list-runs -d ${dag_id}
\`\`\`

## Usage

### Manual Trigger

\`\`\`bash
airflow dags trigger ${dag_id} --conf '{"key": "value"}'
\`\`\`

### Scheduled Execution

This DAG runs automatically on schedule: **${schedule_interval || '@daily'}**

### Parameters

${generateParametersSection(tasks)}

## Monitoring

### View DAG in UI

1. Navigate to Airflow web UI
2. Find \`${dag_id}\` in the DAGs list
3. Click to view execution history and task details

### Logs

Task logs are available in:
- Airflow UI: Graph View → Click Task → View Logs
- File system: \`$AIRFLOW_HOME/logs/${dag_id}/\`

### Alerts

${default_args.email_on_failure || default_args.email_on_retry ? 
`Email alerts are configured:
- **On Failure**: ${default_args.email_on_failure ? 'Yes' : 'No'}
- **On Retry**: ${default_args.email_on_retry ? 'Yes' : 'No'}
- **Recipients**: ${default_args.email ? default_args.email.join(', ') : 'Not configured'}` :
`Configure email alerts by setting \`email_on_failure\` and \`email\` in default_args.`}

## Troubleshooting

### Common Issues

**DAG not appearing in UI**
- Verify DAG file is in \`$AIRFLOW_HOME/dags/\`
- Check for syntax errors: \`python ${dag_id}.py\`
- Review import errors: \`airflow dags list-import-errors\`

**Task failures**
- Check task logs in Airflow UI
- Verify connections are configured correctly
- Ensure required packages are installed

**Schedule not running**
- Verify scheduler is running: \`airflow scheduler\`
- Check DAG is not paused in UI
- Review start_date and schedule_interval

### Support

Generated by DDE Pipeline Generator  
Documentation: https://github.com/your-org/dde

## License

This pipeline was generated automatically. Please review and modify according to your organization's standards.

---

**Generated**: ${new Date().toISOString()}  
${metadata.generatedBy ? `**Generated By**: ${metadata.generatedBy}` : ''}
${metadata.model ? `**AI Model**: ${metadata.model}` : ''}
`;

  return readme;
}

/**
 * Generate task-specific details section
 */
function generateTaskDetails(task) {
  const details = [];
  
  // Add operator-specific parameters
  const params = { ...task };
  delete params.task_id;
  delete params.operator;
  delete params.description;
  
  if (Object.keys(params).length > 0) {
    details.push('**Parameters**:');
    Object.entries(params).forEach(([key, value]) => {
      if (typeof value === 'object') {
        details.push(`- \`${key}\`: \`${JSON.stringify(value)}\``);
      } else {
        details.push(`- \`${key}\`: \`${value}\``);
      }
    });
  }
  
  return details.join('\n');
}

/**
 * Generate dependency graph visualization
 */
function generateDependencyGraph(tasks, dependencies) {
  if (!dependencies || dependencies.length === 0) {
    // Simple linear flow
    return tasks.map(t => t.task_id).join(' >> ');
  }
  
  // Parse custom dependencies
  return dependencies.join('\n');
}

/**
 * Extract environment variables from tasks
 */
function extractEnvironmentVariables(tasks) {
  const envVars = [];
  const seen = new Set();
  
  tasks.forEach(task => {
    // Look for common env var patterns
    const taskStr = JSON.stringify(task);
    const matches = taskStr.match(/\{\{\s*var\.value\.(\w+)\s*\}\}/g) || [];
    
    matches.forEach(match => {
      const varName = match.match(/var\.value\.(\w+)/)[1];
      if (!seen.has(varName)) {
        seen.add(varName);
        envVars.push({
          name: varName,
          description: 'Airflow variable'
        });
      }
    });
  });
  
  return envVars;
}

/**
 * Extract required connections from tasks
 */
function extractConnections(tasks) {
  const connections = [];
  const seen = new Set();
  
  tasks.forEach(task => {
    // Check for common connection ID patterns
    const connKeys = [
      'postgres_conn_id',
      'mysql_conn_id',
      'http_conn_id',
      'aws_conn_id',
      'gcp_conn_id',
      'mongo_conn_id',
      'mssql_conn_id',
      'ssh_conn_id',
      'ftp_conn_id'
    ];
    
    connKeys.forEach(key => {
      if (task[key] && !seen.has(task[key])) {
        seen.add(task[key]);
        const connType = key.replace('_conn_id', '');
        connections.push({
          id: task[key],
          type: connType,
          description: `Required for ${task.operator}`
        });
      }
    });
  });
  
  return connections;
}

/**
 * Generate parameters section
 */
function generateParametersSection(tasks) {
  const hasParams = tasks.some(task => task.params || task.op_kwargs);
  
  if (!hasParams) {
    return 'No runtime parameters required.';
  }
  
  const paramsList = [];
  tasks.forEach(task => {
    if (task.params || task.op_kwargs) {
      paramsList.push(`### ${task.task_id}`);
      const params = task.params || task.op_kwargs;
      Object.entries(params).forEach(([key, value]) => {
        paramsList.push(`- \`${key}\`: ${typeof value === 'object' ? JSON.stringify(value) : value}`);
      });
    }
  });
  
  return paramsList.join('\n');
}

/**
 * Generate requirements.txt content
 */
export function generateRequirements(dag, additionalPackages = []) {
  const requirements = new Set([
    'apache-airflow>=2.0.0',
  ]);
  
  // Add operator-specific requirements
  const tasks = dag.tasks || [];
  tasks.forEach(task => {
    const operator = task.operator || task.operator_type || '';
    
    // Ensure operator is a string
    if (typeof operator !== 'string' || !operator) {
      return; // Skip this task if no valid operator
    }
    
    if (operator.includes('Postgres')) {
      requirements.add('apache-airflow-providers-postgres>=2.0.0');
    } else if (operator.includes('MySQL')) {
      requirements.add('apache-airflow-providers-mysql>=2.0.0');
    } else if (operator.includes('S3') || operator.includes('AWS')) {
      requirements.add('apache-airflow-providers-amazon>=2.0.0');
    } else if (operator.includes('GCS') || operator.includes('BigQuery') || operator.includes('Google')) {
      requirements.add('apache-airflow-providers-google>=6.0.0');
    } else if (operator.includes('Snowflake')) {
      requirements.add('apache-airflow-providers-snowflake>=2.0.0');
    } else if (operator.includes('Mongo')) {
      requirements.add('apache-airflow-providers-mongo>=2.0.0');
    } else if (operator.includes('Docker')) {
      requirements.add('apache-airflow-providers-docker>=2.0.0');
    } else if (operator.includes('Kubernetes')) {
      requirements.add('apache-airflow-providers-cncf-kubernetes>=2.0.0');
    } else if (operator.includes('Spark')) {
      requirements.add('apache-airflow-providers-apache-spark>=2.0.0');
    } else if (operator.includes('Email')) {
      requirements.add('apache-airflow-providers-smtp>=1.0.0');
    } else if (operator.includes('Slack')) {
      requirements.add('apache-airflow-providers-slack>=4.0.0');
    }
  });
  
  // Add additional packages
  additionalPackages.forEach(pkg => requirements.add(pkg));
  
  return Array.from(requirements).sort().join('\n') + '\n';
}

/**
 * Generate deployment instructions
 */
export function generateDeploymentInstructions(dag) {
  return `# Deployment Instructions

## Local Development

1. **Install Airflow**:
   \`\`\`bash
   pip install -r requirements.txt
   \`\`\`

2. **Initialize Airflow**:
   \`\`\`bash
   export AIRFLOW_HOME=~/airflow
   airflow db init
   \`\`\`

3. **Create Admin User**:
   \`\`\`bash
   airflow users create \\
       --username admin \\
       --firstname Admin \\
       --lastname User \\
       --role Admin \\
       --email admin@example.com
   \`\`\`

4. **Deploy DAG**:
   \`\`\`bash
   cp ${dag.dag_id}.py $AIRFLOW_HOME/dags/
   \`\`\`

5. **Start Services**:
   \`\`\`bash
   # Terminal 1: Webserver
   airflow webserver --port 8080
   
   # Terminal 2: Scheduler
   airflow scheduler
   \`\`\`

6. **Access UI**: http://localhost:8080

## Docker Deployment

1. **Using docker-compose.yml** (included in package):
   \`\`\`bash
   docker-compose up -d
   \`\`\`

2. **Access UI**: http://localhost:8080
   - Username: admin
   - Password: admin

## Production Deployment

1. **Use Managed Service**:
   - AWS MWAA (Managed Workflows for Apache Airflow)
   - Google Cloud Composer
   - Astronomer

2. **Self-Hosted Kubernetes**:
   - Use Helm chart: https://airflow.apache.org/docs/helm-chart/
   
3. **CI/CD Integration**:
   - Store DAG in Git repository
   - Automated deployment on merge to main branch

## Configuration Checklist

- [ ] Configure required connections
- [ ] Set environment variables
- [ ] Deploy DAG file
- [ ] Test DAG parsing
- [ ] Test individual tasks
- [ ] Enable DAG in UI
- [ ] Monitor first scheduled run
- [ ] Set up alerting

For detailed instructions, see README.md
`;
}
