"""
DAG Code Generator
Converts DAG JSON specifications to executable Python code
"""

import json
import sys
from datetime import datetime, timedelta


def generate_dag_code(spec):
    """
    Generate Airflow DAG Python code from specification
    
    Args:
        spec (dict): DAG specification
    
    Returns:
        str: Generated Python code
    """
    dag_id = spec.get('dag_id', 'generated_dag')
    description = spec.get('description', '')
    schedule_interval = spec.get('schedule_interval', '@daily')
    start_date = spec.get('start_date', datetime.now().strftime('%Y-%m-%d'))
    tags = spec.get('tags', [])
    tasks = spec.get('tasks', [])
    default_args = spec.get('default_args', {})
    dependencies = spec.get('dependencies', [])
    
    # Build default_args
    if not default_args:
        default_args = {
            'owner': 'airflow',
            'depends_on_past': False,
            'retries': 1,
            'retry_delay': timedelta(minutes=5)
        }
    
    # Generate imports based on operators used
    operators_used = set([task.get('operator', 'PythonOperator') for task in tasks])
    imports = ['from airflow import DAG']
    imports.append('from datetime import datetime, timedelta')
    
    # Map operators to imports
    operator_imports = {
        'PythonOperator': 'from airflow.operators.python import PythonOperator',
        'BashOperator': 'from airflow.operators.bash import BashOperator',
        'PostgresOperator': 'from airflow.providers.postgres.operators.postgres import PostgresOperator',
        'MySqlOperator': 'from airflow.providers.mysql.operators.mysql import MySqlOperator',
        'S3Operator': 'from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator',
        'EmailOperator': 'from airflow.operators.email import EmailOperator',
        'SlackOperator': 'from airflow.providers.slack.operators.slack import SlackAPIPostOperator',
        'HttpOperator': 'from airflow.providers.http.operators.http import SimpleHttpOperator',
        'DockerOperator': 'from airflow.providers.docker.operators.docker import DockerOperator',
        'BranchPythonOperator': 'from airflow.operators.python import BranchPythonOperator',
    }
    
    for operator in operators_used:
        if operator in operator_imports:
            imports.append(operator_imports[operator])
    
    # Start building code
    code_lines = []
    
    # Add header comment
    code_lines.append('"""')
    code_lines.append(f'{dag_id}')
    code_lines.append('')
    code_lines.append(f'{description}')
    code_lines.append('')
    code_lines.append('Generated by DDE Pipeline Generator')
    code_lines.append(f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    code_lines.append('"""')
    code_lines.append('')
    
    # Add imports
    code_lines.extend(imports)
    code_lines.append('')
    code_lines.append('')
    
    # Add default_args
    code_lines.append('default_args = {')
    for key, value in default_args.items():
        if isinstance(value, str):
            code_lines.append(f"    '{key}': '{value}',")
        elif isinstance(value, bool):
            code_lines.append(f"    '{key}': {value},")
        elif isinstance(value, timedelta):
            code_lines.append(f"    '{key}': timedelta(seconds={value.total_seconds()}),")
        elif isinstance(value, list):
            code_lines.append(f"    '{key}': {value},")
        else:
            code_lines.append(f"    '{key}': {value},")
    code_lines.append('}')
    code_lines.append('')
    
    # Create DAG
    code_lines.append('# Create DAG')
    code_lines.append(f"dag = DAG(")
    code_lines.append(f"    '{dag_id}',")
    code_lines.append(f"    default_args=default_args,")
    code_lines.append(f"    description='{description}',")
    code_lines.append(f"    schedule_interval='{schedule_interval}',")
    code_lines.append(f"    start_date=datetime({start_date.split('-')[0]}, {start_date.split('-')[1]}, {start_date.split('-')[2]}),")
    if tags:
        code_lines.append(f"    tags={tags},")
    code_lines.append(f"    catchup=False,")
    code_lines.append(f")")
    code_lines.append('')
    
    # Add task definitions
    code_lines.append('# Task definitions')
    for task in tasks:
        task_id = task.get('task_id')
        operator = task.get('operator', 'PythonOperator')
        
        code_lines.append(f"{task_id} = {operator}(")
        code_lines.append(f"    task_id='{task_id}',")
        
        # Add operator-specific parameters
        for key, value in task.items():
            if key not in ['task_id', 'operator', 'description']:
                if isinstance(value, str):
                    code_lines.append(f"    {key}='{value}',")
                elif isinstance(value, (list, dict)):
                    code_lines.append(f"    {key}={value},")
                else:
                    code_lines.append(f"    {key}={value},")
        
        code_lines.append(f"    dag=dag")
        code_lines.append(f")")
        code_lines.append('')
    
    # Add dependencies
    if dependencies:
        code_lines.append('# Task dependencies')
        for dep in dependencies:
            code_lines.append(dep)
    else:
        # Create simple linear flow
        if len(tasks) > 1:
            code_lines.append('# Task dependencies (linear flow)')
            task_ids = [task.get('task_id') for task in tasks]
            code_lines.append(' >> '.join(task_ids))
    
    return '\n'.join(code_lines)


if __name__ == '__main__':
    # Read from stdin
    spec_json = sys.stdin.read()
    spec = json.loads(spec_json)
    
    # Generate code
    code = generate_dag_code(spec)
    
    # Output code
    print(code)
