"""
DAG Code Generator
Converts JSON pipeline specification to executable Airflow DAG Python code.
"""

from datetime import datetime
from typing import Dict, List, Any, Optional
import json


class DAGCodeGenerator:
    """Generates Python DAG code from JSON specification."""
    
    # Operator import mappings
    OPERATOR_IMPORTS = {
        'BashOperator': 'from airflow.operators.bash import BashOperator',
        'PythonOperator': 'from airflow.operators.python import PythonOperator',
        'EmailOperator': 'from airflow.operators.email import EmailOperator',
        'DummyOperator': 'from airflow.operators.dummy import DummyOperator',
        'EmptyOperator': 'from airflow.operators.empty import EmptyOperator',
        'BranchPythonOperator': 'from airflow.operators.python import BranchPythonOperator',
        'ShortCircuitOperator': 'from airflow.operators.python import ShortCircuitOperator',
        'PostgresOperator': 'from airflow.providers.postgres.operators.postgres import PostgresOperator',
        'MySqlOperator': 'from airflow.providers.mysql.operators.mysql import MySqlOperator',
        'SqliteOperator': 'from airflow.providers.sqlite.operators.sqlite import SqliteOperator',
        'S3CreateBucketOperator': 'from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator',
        'S3DeleteBucketOperator': 'from airflow.providers.amazon.aws.operators.s3 import S3DeleteBucketOperator',
        'S3CreateObjectOperator': 'from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator',
        'S3DeleteObjectsOperator': 'from airflow.providers.amazon.aws.operators.s3 import S3DeleteObjectsOperator',
        'GCSCreateBucketOperator': 'from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator',
        'GCSDeleteBucketOperator': 'from airflow.providers.google.cloud.operators.gcs import GCSDeleteBucketOperator',
        'GCSObjectCreateAclEntryOperator': 'from airflow.providers.google.cloud.operators.gcs import GCSObjectCreateAclEntryOperator',
        'BigQueryCreateEmptyDatasetOperator': 'from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyDatasetOperator',
        'BigQueryDeleteDatasetOperator': 'from airflow.providers.google.cloud.operators.bigquery import BigQueryDeleteDatasetOperator',
        'BigQueryInsertJobOperator': 'from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator',
        'HttpOperator': 'from airflow.providers.http.operators.http import HttpOperator',
        'SimpleHttpOperator': 'from airflow.providers.http.operators.http import SimpleHttpOperator',
        'DockerOperator': 'from airflow.providers.docker.operators.docker import DockerOperator',
        'KubernetesPodOperator': 'from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator',
        'SparkSubmitOperator': 'from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator',
        'DataprocSubmitJobOperator': 'from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator',
        'SlackAPIPostOperator': 'from airflow.providers.slack.operators.slack import SlackAPIPostOperator',
        'SFTPOperator': 'from airflow.providers.sftp.operators.sftp import SFTPOperator',
        'FTPFileTransmitOperator': 'from airflow.providers.ftp.operators.ftp import FTPFileTransmitOperator',
    }
    
    def __init__(self, spec: Dict[str, Any]):
        """Initialize with pipeline specification."""
        self.spec = spec
        self.indent = '    '
        
    def generate(self) -> str:
        """Generate complete DAG Python code."""
        lines = []
        
        # Add header comment
        lines.extend(self._generate_header())
        lines.append('')
        
        # Add imports
        lines.extend(self._generate_imports())
        lines.append('')
        
        # Add default args
        lines.extend(self._generate_default_args())
        lines.append('')
        
        # Add DAG definition
        lines.extend(self._generate_dag_definition())
        lines.append('')
        
        # Add tasks
        lines.extend(self._generate_tasks())
        lines.append('')
        
        # Add dependencies
        lines.extend(self._generate_dependencies())
        
        return '\n'.join(lines)
    
    def _generate_header(self) -> List[str]:
        """Generate file header with metadata."""
        lines = [
            '"""',
            f'DAG: {self.spec.get("dag_id", "generated_dag")}',
            f'Description: {self.spec.get("description", "Auto-generated Airflow DAG")}',
            '',
            'Generated by DDE Pipeline Generator',
            f'Generated at: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
            '"""',
        ]
        return lines
    
    def _generate_imports(self) -> List[str]:
        """Generate import statements."""
        lines = [
            'from datetime import datetime, timedelta',
            'from airflow import DAG',
        ]
        
        # Collect unique operators
        operators = set()
        for task in self.spec.get('tasks', []):
            op_type = task.get('operator_type') or task.get('operator')
            if op_type:
                operators.add(op_type)
        
        # Add operator imports
        for op in sorted(operators):
            if op in self.OPERATOR_IMPORTS:
                lines.append(self.OPERATOR_IMPORTS[op])
        
        return lines
    
    def _generate_default_args(self) -> List[str]:
        """Generate default_args dictionary."""
        lines = ['default_args = {']
        
        default_args = self.spec.get('default_args', {})
        
        # Add owner
        owner = default_args.get('owner', 'airflow')
        lines.append(f"{self.indent}'owner': '{owner}',")
        
        # Add depends_on_past
        if 'depends_on_past' in default_args:
            lines.append(f"{self.indent}'depends_on_past': {default_args['depends_on_past']},")
        
        # Add email settings
        if 'email' in default_args:
            email = default_args['email']
            if isinstance(email, list):
                email_str = str(email)
            else:
                email_str = f"['{email}']"
            lines.append(f"{self.indent}'email': {email_str},")
        
        if 'email_on_failure' in default_args:
            lines.append(f"{self.indent}'email_on_failure': {default_args['email_on_failure']},")
        
        if 'email_on_retry' in default_args:
            lines.append(f"{self.indent}'email_on_retry': {default_args['email_on_retry']},")
        
        # Add retries
        retries = default_args.get('retries', 1)
        lines.append(f"{self.indent}'retries': {retries},")
        
        # Add retry_delay
        retry_delay = default_args.get('retry_delay', 300)
        lines.append(f"{self.indent}'retry_delay': timedelta(seconds={retry_delay}),")
        
        lines.append('}')
        return lines
    
    def _generate_dag_definition(self) -> List[str]:
        """Generate DAG instantiation."""
        lines = ['with DAG(']
        
        # DAG ID
        dag_id = self.spec.get('dag_id', 'generated_dag')
        lines.append(f"{self.indent}dag_id='{dag_id}',")
        
        # Description
        description = self.spec.get('description', 'Auto-generated DAG')
        lines.append(f"{self.indent}description='{description}',")
        
        # Default args
        lines.append(f"{self.indent}default_args=default_args,")
        
        # Start date
        start_date = self.spec.get('start_date', '2024-01-01')
        lines.append(f"{self.indent}start_date=datetime({start_date.replace('-', ', ')}),")
        
        # Schedule
        schedule = self.spec.get('schedule', '@daily')
        if schedule:
            lines.append(f"{self.indent}schedule='{schedule}',")
        else:
            lines.append(f"{self.indent}schedule=None,")
        
        # Catchup
        catchup = self.spec.get('catchup', False)
        lines.append(f"{self.indent}catchup={catchup},")
        
        # Tags
        tags = self.spec.get('tags', [])
        if tags:
            tags_str = str(tags)
            lines.append(f"{self.indent}tags={tags_str},")
        
        lines.append(') as dag:')
        return lines
    
    def _generate_tasks(self) -> List[str]:
        """Generate task definitions."""
        lines = []
        tasks = self.spec.get('tasks', [])
        
        for i, task in enumerate(tasks):
            if i > 0:
                lines.append('')  # Blank line between tasks
            
            task_id = task.get('task_id')
            op_type = task.get('operator_type') or task.get('operator')
            
            lines.append(f"{self.indent}{task_id} = {op_type}(")
            lines.append(f"{self.indent * 2}task_id='{task_id}',")
            
            # Add operator-specific parameters
            params = task.get('parameters', {})
            
            # Handle common parameters
            for key, value in params.items():
                if key in ['task_id', 'operator_type', 'operator']:
                    continue
                    
                if isinstance(value, str):
                    # Check if it's a Python expression (starts with py:)
                    if value.startswith('py:'):
                        lines.append(f"{self.indent * 2}{key}={value[3:]},")
                    else:
                        lines.append(f"{self.indent * 2}{key}='{value}',")
                elif isinstance(value, bool):
                    lines.append(f"{self.indent * 2}{key}={value},")
                elif isinstance(value, (int, float)):
                    lines.append(f"{self.indent * 2}{key}={value},")
                elif isinstance(value, list):
                    lines.append(f"{self.indent * 2}{key}={value},")
                elif isinstance(value, dict):
                    lines.append(f"{self.indent * 2}{key}={value},")
            
            # Add common task-level parameters
            if task.get('retries') is not None:
                lines.append(f"{self.indent * 2}retries={task['retries']},")
            
            if task.get('retry_delay') is not None:
                lines.append(f"{self.indent * 2}retry_delay=timedelta(seconds={task['retry_delay']}),")
            
            if task.get('pool'):
                lines.append(f"{self.indent * 2}pool='{task['pool']}',")
            
            if task.get('priority_weight') is not None:
                lines.append(f"{self.indent * 2}priority_weight={task['priority_weight']},")
            
            lines.append(f"{self.indent})")
        
        return lines
    
    def _generate_dependencies(self) -> List[str]:
        """Generate task dependencies."""
        lines = []
        tasks = self.spec.get('tasks', [])
        
        # Build dependency graph
        dependencies = {}
        for task in tasks:
            task_id = task.get('task_id')
            deps = task.get('dependencies', [])
            if deps:
                dependencies[task_id] = deps
        
        if not dependencies:
            return lines
        
        lines.append('# Task dependencies')
        
        for task_id, deps in dependencies.items():
            if len(deps) == 1:
                lines.append(f"{deps[0]} >> {task_id}")
            else:
                # Multiple dependencies
                deps_str = '[' + ', '.join(deps) + ']'
                lines.append(f"{deps_str} >> {task_id}")
        
        return lines
    
    def save_to_file(self, filepath: str) -> None:
        """Save generated code to file."""
        code = self.generate()
        with open(filepath, 'w') as f:
            f.write(code)


def generate_dag_code(spec: Dict[str, Any]) -> str:
    """
    Generate DAG Python code from specification.
    
    Args:
        spec: Pipeline specification dictionary
        
    Returns:
        Generated Python code as string
    """
    generator = DAGCodeGenerator(spec)
    return generator.generate()


if __name__ == '__main__':
    # Test with sample spec
    sample_spec = {
        "dag_id": "example_etl_pipeline",
        "description": "Example ETL pipeline for data processing",
        "schedule": "@daily",
        "start_date": "2024-01-01",
        "catchup": False,
        "tags": ["etl", "example"],
        "default_args": {
            "owner": "data_team",
            "retries": 2,
            "retry_delay": 300,
            "email": ["admin@example.com"],
            "email_on_failure": True
        },
        "tasks": [
            {
                "task_id": "extract_data",
                "operator_type": "BashOperator",
                "parameters": {
                    "bash_command": "echo 'Extracting data...'"
                }
            },
            {
                "task_id": "transform_data",
                "operator_type": "PythonOperator",
                "parameters": {
                    "python_callable": "py:transform_function"
                },
                "dependencies": ["extract_data"]
            },
            {
                "task_id": "load_data",
                "operator_type": "BashOperator",
                "parameters": {
                    "bash_command": "echo 'Loading data...'"
                },
                "dependencies": ["transform_data"]
            }
        ]
    }
    
    code = generate_dag_code(sample_spec)
    print(code)
